{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBase:\n",
    "    def __init__(self, id, n_options=2, q_init=0.1):\n",
    "        # properties\n",
    "        self.id = id\n",
    "\n",
    "        # arrays to hold behavioural history\n",
    "        self.Q_vals = [np.array([q_init for i in range(n_options)])]\n",
    "        self.choices = []\n",
    "        self.correct = []\n",
    "        self.payoffs = []\n",
    "        \n",
    "        self.n_actions = n_options # TODO hmm\n",
    "\n",
    "    def update_state(self, reward, correct):\n",
    "        self.payoffs.append(reward)\n",
    "        self.correct.append(correct)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Agent {self.id}\"\n",
    "    \n",
    "\n",
    "class AgentEWA(AgentBase):\n",
    "    def __init__(self, id, phi=0.1, lam=2):\n",
    "        super().__init__(id)\n",
    "        self.phi = phi\n",
    "        self.lam = lam\n",
    "        self.choice_f = lambda x: np.exp(x)/sum(np.exp(x)) # TODO parameter?\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        choice_probabilities = self.choice_f(self.Q_vals[-1])\n",
    "        choice = np.random.choice(self.n_actions, 1, p=choice_probabilities).item()\n",
    "        self.choices.append(choice)\n",
    "        \n",
    "        return choice\n",
    "\n",
    "    def update_Qvals(self, choice, reward):\n",
    "        last_q = self.Q_vals[-1].copy() # get most recent Qvals\n",
    "        #update all choices?\n",
    "        for idx, val in enumerate(last_q):\n",
    "            if idx == choice:\n",
    "                last_q[idx] = (1 - self.phi) * last_q[choice] + self.phi * reward \n",
    "            else:\n",
    "                last_q[idx] = (1 - self.phi) * last_q[choice] + self.phi * 0\n",
    "\n",
    "        self.Q_vals.append(last_q)\n",
    "    \n",
    "\n",
    "class AgentQ(AgentBase):\n",
    "    def __init__(self, id, epsilon=0.5):\n",
    "        super().__init__(id)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        q_vals_current = self.Q_vals[-1]\n",
    "        p = np.random.uniform()\n",
    "        # choose random with p=epsilon otherwise greedy\n",
    "        if p < self.epsilon:\n",
    "            choice = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            choice = np.argmax(q_vals_current)\n",
    "\n",
    "        self.choices.append(choice)\n",
    "        \n",
    "        return int(choice)\n",
    "    \n",
    "    def update_Qvals(self, choice, reward):\n",
    "        last_q = self.Q_vals[-1].copy() # get most recent Qvals\n",
    "        chosen_q = last_q[choice]\n",
    "        new_q = chosen_q + (reward - chosen_q) / len(self.Q_vals) # == n of steps, >= 1\n",
    "        last_q[choice] = new_q\n",
    "        \n",
    "        self.Q_vals.append(last_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, \n",
    "                 n_agents=1, \n",
    "                 n_options=2, \n",
    "                 payoff_structure=(0.6, 0.59, 0)\n",
    "                 ):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_options = n_options\n",
    "        # payoffs (rewards)\n",
    "        self.payoff_better, self.payoff_worse, \\\n",
    "            self.payoff_sd = payoff_structure\n",
    "        self.best_action = np.random.randint(0, n_options) # best action drawn at random\n",
    "\n",
    "    def return_rewards(self, choice_idx: int) -> tuple[float, bool]:\n",
    "        assert isinstance(choice_idx, int) and (choice_idx < self.n_options)\n",
    "        # TODO\n",
    "        better = np.random.normal(self.payoff_better, self.payoff_sd)\n",
    "        worse = np.random.normal(self.payoff_worse, self.payoff_sd)\n",
    "\n",
    "        return (better, True) if (choice_idx == self.best_action) else (worse, False)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"MAB env\\nAgents: {self.n_agents}\\nSize of action space: {self.n_options}\n",
    "Best idx: {self.best_action}\n",
    "Rewards (high, low, SD): {self.payoff_better, self.payoff_worse, self.payoff_sd}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6       , 0.45437954])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Env()\n",
    "a = AgentQ(\"A\")\n",
    "\n",
    "for i in range(1000):\n",
    "    choice = a.choose_action()\n",
    "    reward, is_optimal = e.return_rewards(choice)\n",
    "    a.update_state(reward, is_optimal)\n",
    "    a.update_Qvals(choice, reward)\n",
    "\n",
    "\n",
    "a.Q_vals[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'A',\n",
       " 'Q_vals': [array([0.1, 0.1]),\n",
       "  array([0.59, 0.1 ]),\n",
       "  array([0.59, 0.1 ]),\n",
       "  array([0.59      , 0.26666667])],\n",
       " 'choices': [0, 0, 1, 0],\n",
       " 'correct': [False, False, True],\n",
       " 'payoffs': [0.59, 0.59, 0.6],\n",
       " 'n_actions': 2,\n",
       " 'epsilon': 0.5}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reverend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
