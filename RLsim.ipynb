{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 0.1), (3.3, 3.3)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EWAagent:\n",
    "    def __init__(self, id, phi=0.1, lam=2, n_options=2, q_init=0.1):\n",
    "        # properties\n",
    "        self.id = id\n",
    "        self.phi = phi\n",
    "        self.lam = lam\n",
    "        self.choice_f = lambda x: np.exp(x)/sum(np.exp(x)) # TODO parameter?\n",
    "\n",
    "        # arrays to hold behavioural history\n",
    "        self.Q_vals = [tuple(q_init for i  in range(n_options))]\n",
    "        self.choices = []\n",
    "        self.correct = []\n",
    "        self.payoffs = []\n",
    "        \n",
    "        self.n_actions = n_options # TODO hmm\n",
    "\n",
    "    def update_Q_vals(self, values: tuple[float, ...]):\n",
    "        self.Q_vals.append(values)\n",
    "\n",
    "    def choose_action(self) -> int:\n",
    "        choice_probabilities = self.choice_f(self.Q_vals[-1])\n",
    "        choice = np.random.choice(self.n_actions, 1, p=choice_probabilities).item()\n",
    "        self.choices.append(choice)\n",
    "        return choice\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Agent {self.id}\"\n",
    "\n",
    "\n",
    "ags = [EWAagent(i) for i in range(3)]\n",
    "ags[0].update_Q_vals((3.3,3.3))\n",
    "ags[0].Q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88079708, 0.11920292])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "softmax((4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAB env\n",
       "Agents: 1\n",
       "Size of action space: 2\n",
       "Rewards (high, low, SD): (0.6, 0.59, 0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Env:\n",
    "    def __init__(self, \n",
    "                 n_agents=1, \n",
    "                 n_options=2, \n",
    "                 payoff_structure=(0.6, 0.59, 0)\n",
    "                 ):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_options = n_options\n",
    "        # payoffs\n",
    "        self.payoff_better, self.payoff_worse, \\\n",
    "            self.payoff_sd = payoff_structure\n",
    "\n",
    "\n",
    "    def return_rewards(self, choice: int) -> float:\n",
    "        # TODO\n",
    "        better = np.random.normal(self.payoff_better, self.payoff_sd)\n",
    "        worse = np.random.normal(self.payoff_worse, self.payoff_sd)\n",
    "\n",
    "        return better if (choice==1) else worse\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"MAB env\\nAgents: {self.n_agents}\\nSize of action space: {self.n_options}\n",
    "Rewards (high, low, SD): {self.payoff_better, self.payoff_worse, self.payoff_sd}\"\"\"\n",
    "\n",
    "e = Env()\n",
    "a = EWAagent(\"A\")\n",
    "\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim(\n",
    "        Tmax=100, \n",
    "        n_agents=1, \n",
    "        n_options=2,\n",
    "        payoff_better=0.6,\n",
    "        payoff_worse=0.59,\n",
    "        payoff_sd=0,\n",
    "        phi=0.1,\n",
    "        lam=2):\n",
    "    \n",
    "        agents = [EWAagent(i) for i in range(n_agents)]\n",
    "        actions = [payoff_better, payoff_worse]\n",
    "\n",
    "        for trial in range(Tmax):\n",
    "                for agent in agents:\n",
    "                        # choose action\n",
    "                        choice = agent.choose_action()\n",
    "                        \n",
    "                        \n",
    "        return agents[0].choices\n",
    "\n",
    "choices = sim()\n",
    "np.mean(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full([4,3], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reverend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
