{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWAagent:\n",
    "    def __init__(self, id, phi=0.1, lam=2, n_options=2, q_init=0.1):\n",
    "        # properties\n",
    "        self.id = id\n",
    "        self.phi = phi\n",
    "        self.lam = lam\n",
    "        self.choice_f = lambda x: np.exp(x)/sum(np.exp(x)) # TODO parameter?\n",
    "\n",
    "        # arrays to hold behavioural history\n",
    "        self.Q_vals = [np.array([q_init for i in range(n_options)])]\n",
    "        self.choices = []\n",
    "        self.correct = []\n",
    "        self.payoffs = []\n",
    "        \n",
    "        self.n_actions = n_options # TODO hmm\n",
    "\n",
    "    def update_Q_vals(self, choice, reward):\n",
    "        last_q = self.Q_vals[-1].copy() # get current rewards\n",
    "        #update all choices?\n",
    "        for idx, val in enumerate(last_q):\n",
    "            if idx == choice:\n",
    "                last_q[idx] = (1 - self.phi) * last_q[choice] + self.phi * reward \n",
    "            else:\n",
    "                last_q[idx] = (1 - self.phi) * last_q[choice] + self.phi * 0\n",
    "        \n",
    "        self.Q_vals.append(last_q)\n",
    "\n",
    "    def update_state(self, reward, correct):\n",
    "        self.payoffs.append(reward)\n",
    "        self.correct.append(correct)\n",
    "        \n",
    "    def choose_action(self) -> int:\n",
    "        choice_probabilities = self.choice_f(self.Q_vals[-1])\n",
    "        choice = np.random.choice(self.n_actions, 1, p=choice_probabilities).item()\n",
    "        self.choices.append(choice)\n",
    "        \n",
    "        return choice\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Agent {self.id}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([np.random.randint(0, 3) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, \n",
    "                 n_agents=1, \n",
    "                 n_options=2, \n",
    "                 payoff_structure=(0.6, 0.59, 0)\n",
    "                 ):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_options = n_options\n",
    "        # payoffs\n",
    "        self.payoff_better, self.payoff_worse, \\\n",
    "            self.payoff_sd = payoff_structure\n",
    "        self.optimal_action = np.random.randint(0, n_options) # best action drawn at random\n",
    "\n",
    "    def return_rewards(self, choice_idx: int) -> tuple[float, bool]:\n",
    "        assert isinstance(choice_idx, int) and (choice_idx < self.n_options)\n",
    "        # TODO\n",
    "        better = np.random.normal(self.payoff_better, self.payoff_sd)\n",
    "        worse = np.random.normal(self.payoff_worse, self.payoff_sd)\n",
    "\n",
    "        return (better, True) if (choice_idx == self.optimal_action) else (worse, False)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"MAB env\\nAgents: {self.n_agents}\\nSize of action space: {self.n_options}\n",
    "Rewards (high, low, SD): {self.payoff_better, self.payoff_worse, self.payoff_sd}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35472462, 0.41472462])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Env()\n",
    "a = EWAagent(\"A\")\n",
    "\n",
    "for i in range(1000):\n",
    "    choice = a.choose_action()\n",
    "    reward, is_optimal = e.return_rewards(choice)\n",
    "    a.update_state(reward, is_optimal)\n",
    "    a.update_Q_vals(choice, reward)\n",
    "\n",
    "\n",
    "a.Q_vals[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full([4,3], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reverend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
